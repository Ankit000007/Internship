{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 3 : Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Importing selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "#Importing requests\n",
    "import requests\n",
    "\n",
    "# importing regex\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q1- Write a python program which searches all the product under a particular product vertical\n",
    "from www.amazon.in. The product verticals to be searched will be taken as input from user.\n",
    "For e.g. If user input is ‘guitar’. Then search for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search : slipper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")    \n",
    "search_bar.clear()                                               \n",
    "search_bar.send_keys(user_inp)                                   \n",
    "search_button = driver.find_element_by_xpath('//div[@class=\"nav-search-submit nav-sprite\"]/span/input')       \n",
    "search_button.click()                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q2- In the above question, now scrape the following details of each product listed in first 3 pages \n",
    "of your search results and save it in a dataframe and csv. In case if any product vertical has \n",
    "less than 3 pages in search results then scrape all the products available under that product \n",
    "vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of \n",
    "Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\" \n",
    "and “Product URL”. In case, if any of the details are missing for any of the product then \n",
    "replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "\n",
    "time.sleep(2)\n",
    "user_inp = input('Enter the product you want to search : ')\n",
    "search_bar = driver.find_element_by_id(\"twotabsearchtextbox\")    \n",
    "search_bar.clear()                                               \n",
    "search_bar.send_keys(user_inp)                                   \n",
    "search_button = driver.find_element_by_xpath('//div[@class=\"nav-search-submit nav-sprite\"]/span/input')       \n",
    "search_button.click()  \n",
    "\n",
    "product_urls = []\n",
    "start=0\n",
    "end=3\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')      \n",
    "    for i in url:\n",
    "        product_urls.append(i.get_attribute(\"href\"))                       \n",
    "    nxt_button=driver.find_element_by_xpath(\"//li[@class='a-last']//a\")     \n",
    "    nxt_button.click()\n",
    "    time.sleep(2)\n",
    "\n",
    "#Make empty lists\n",
    "Brand = []   \n",
    "Name = []\n",
    "Rating = []\n",
    "no_of_ratings = []\n",
    "Price = []\n",
    "Return = []\n",
    "expected_delivery = []\n",
    "Availability = [] \n",
    "Other_Details = []\n",
    "\n",
    "for url in product_urls:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extracting Brand from xpath\n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')      \n",
    "        Brand.append(brand.text)\n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')    \n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting name from id\n",
    "    try:\n",
    "        name = driver.find_element_by_id('productTitle')      \n",
    "        Name.append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        Name.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)    \n",
    "    # Extracting Ratings from xpath    \n",
    "    try:\n",
    "        rating = driver.find_element_by_xpath('//span[@class=\"a-size-base a-nowrap\"]//span')  \n",
    "        Rating.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        Rating.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting No of Ratings from xpath\n",
    "    try:\n",
    "        no_rating = driver.find_element_by_xpath('//span[@class=\"a-size-base a-color-secondary\"]')  \n",
    "        no_of_ratings.append(no_rating.text)\n",
    "    except NoSuchElementException:\n",
    "        no_of_ratings.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting price from xpath    \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//td[@class=\"a-span12\"]')  \n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting Return from xpath\n",
    "    try:\n",
    "        return_ = driver.find_element_by_xpath('//a[@class=\"a-size-small a-link-normal a-text-normal\"]')  \n",
    "        Return.append(return_.text)\n",
    "    except NoSuchElementException:\n",
    "        Return.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting expected_delivery from xpath\n",
    "    try:\n",
    "        exptd_dlvry= driver.find_element_by_xpath('//span[@class=\"a-color-error\"]')  \n",
    "        expected_delivery.append(exptd_dlvry.text)\n",
    "    except NoSuchElementException:\n",
    "         expected_delivery.append('-')\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting Availability from xpath         \n",
    "    try:\n",
    "        avl= driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-success\"]')  \n",
    "        Availability.append(avl.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append('-')\n",
    "\n",
    "\n",
    "\n",
    "    time.sleep(1)\n",
    "    # Extracting Other Details from xpath    \n",
    "    try:\n",
    "        othr_dtls= driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')  \n",
    "        Other_Details.append(othr_dtls.text)\n",
    "    except NoSuchElementException:\n",
    "        Other_Details.append('-')\n",
    "\n",
    "time.sleep(2)    \n",
    "# Make dataframe\n",
    "prod_df = pd.DataFrame({'Brand':Brand,'Name':Name,'Rating':Rating,'No. of ratings':no_of_ratings,'Price':Price,\n",
    "                        'Return/Exchange':Return,'Expected Delivery':expected_delivery,'Availability':Availability,\n",
    "                        'Other Details':Other_Details,'URL':product_urls})\n",
    "prod_df.head(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 3-Write a python program to access the search bar and search button on images.google.com and \n",
    "scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 0 of 100 images\n",
      "Downloading 1 of 100 images\n",
      "Downloading 2 of 100 images\n",
      "Downloading 3 of 100 images\n",
      "Downloading 4 of 100 images\n",
      "Downloading 5 of 100 images\n",
      "Downloading 6 of 100 images\n",
      "Downloading 7 of 100 images\n",
      "Downloading 8 of 100 images\n",
      "Downloading 9 of 100 images\n",
      "Downloading 10 of 100 images\n",
      "Downloading 11 of 100 images\n",
      "Downloading 12 of 100 images\n",
      "Downloading 13 of 100 images\n",
      "Downloading 14 of 100 images\n",
      "Downloading 15 of 100 images\n",
      "Downloading 16 of 100 images\n",
      "Downloading 17 of 100 images\n",
      "Downloading 18 of 100 images\n",
      "Downloading 19 of 100 images\n",
      "Downloading 20 of 100 images\n",
      "Downloading 21 of 100 images\n",
      "Downloading 22 of 100 images\n",
      "Downloading 23 of 100 images\n",
      "Downloading 24 of 100 images\n",
      "Downloading 25 of 100 images\n",
      "Downloading 26 of 100 images\n",
      "Downloading 27 of 100 images\n",
      "Downloading 28 of 100 images\n",
      "Downloading 29 of 100 images\n",
      "Downloading 30 of 100 images\n",
      "Downloading 31 of 100 images\n",
      "Downloading 32 of 100 images\n",
      "Downloading 33 of 100 images\n",
      "Downloading 34 of 100 images\n",
      "Downloading 35 of 100 images\n",
      "Downloading 36 of 100 images\n",
      "Downloading 37 of 100 images\n",
      "Downloading 38 of 100 images\n",
      "Downloading 39 of 100 images\n",
      "Downloading 40 of 100 images\n",
      "Downloading 41 of 100 images\n",
      "Downloading 42 of 100 images\n",
      "Downloading 43 of 100 images\n",
      "Downloading 44 of 100 images\n",
      "Downloading 45 of 100 images\n",
      "Downloading 46 of 100 images\n",
      "Downloading 47 of 100 images\n",
      "Downloading 48 of 100 images\n",
      "Downloading 49 of 100 images\n",
      "Downloading 50 of 100 images\n",
      "Downloading 51 of 100 images\n",
      "Downloading 52 of 100 images\n",
      "Downloading 53 of 100 images\n",
      "Downloading 54 of 100 images\n",
      "Downloading 55 of 100 images\n",
      "Downloading 56 of 100 images\n",
      "Downloading 57 of 100 images\n",
      "Downloading 58 of 100 images\n",
      "Downloading 59 of 100 images\n",
      "Downloading 60 of 100 images\n",
      "Downloading 61 of 100 images\n",
      "Downloading 62 of 100 images\n",
      "Downloading 63 of 100 images\n",
      "Downloading 64 of 100 images\n",
      "Downloading 65 of 100 images\n",
      "Downloading 66 of 100 images\n",
      "Downloading 67 of 100 images\n",
      "Downloading 68 of 100 images\n",
      "Downloading 69 of 100 images\n",
      "Downloading 70 of 100 images\n",
      "Downloading 71 of 100 images\n",
      "Downloading 72 of 100 images\n",
      "Downloading 73 of 100 images\n",
      "Downloading 74 of 100 images\n",
      "Downloading 75 of 100 images\n",
      "Downloading 76 of 100 images\n",
      "Downloading 77 of 100 images\n",
      "Downloading 78 of 100 images\n",
      "Downloading 79 of 100 images\n",
      "Downloading 80 of 100 images\n",
      "Downloading 81 of 100 images\n",
      "Downloading 82 of 100 images\n",
      "Downloading 83 of 100 images\n",
      "Downloading 84 of 100 images\n",
      "Downloading 85 of 100 images\n",
      "Downloading 86 of 100 images\n",
      "Downloading 87 of 100 images\n",
      "Downloading 88 of 100 images\n",
      "Downloading 89 of 100 images\n",
      "Downloading 90 of 100 images\n",
      "Downloading 91 of 100 images\n",
      "Downloading 92 of 100 images\n",
      "Downloading 93 of 100 images\n",
      "Downloading 94 of 100 images\n",
      "Downloading 95 of 100 images\n",
      "Downloading 96 of 100 images\n",
      "Downloading 97 of 100 images\n",
      "Downloading 98 of 100 images\n",
      "Downloading 99 of 100 images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://images.google.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "search=input('enter the name of image you want to download')\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/div/div[2]/input')    \n",
    "search_bar.send_keys(search)      \n",
    "search_button = driver.find_element_by_xpath('//*[@id=\"sbtc\"]/button')   \n",
    "search_button.click()        \n",
    "\n",
    "for _ in range(500):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")\n",
    "    \n",
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Rajput Ankit singh\\Desktop\\images\\fruits\"+str(i)+\".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "driver.get('https://www.flipkart.com/')\n",
    "time.sleep(3)\n",
    "try:\n",
    "    login_X_button = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]')                    \n",
    "    login_X_button.click()\n",
    "except NoSuchElementException : \n",
    "    print(\"No Login page\")\n",
    "search_bar = driver.find_element_by_xpath('//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/div[2]/form/div/div/input')  \n",
    "search_bar.clear()              \n",
    "search_bar.send_keys(\"pixel\")      \n",
    "search_button = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')   \n",
    "search_button.click()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip_urls = []\n",
    "urls = driver.find_elements_by_xpath('//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    flip_urls.append(url.get_attribute(\"href\"))\n",
    "    \n",
    "    \n",
    "    \n",
    "#Make empty lists   \n",
    "flip_dict = {}\n",
    "flip_dict[\"Brand\"] = []\n",
    "flip_dict[\"Smartphone\"] = []\n",
    "flip_dict[\"Colour\"] = []\n",
    "flip_dict[\"RAM\"] = []\n",
    "flip_dict[\"Storage(ROM)\"] = []\n",
    "flip_dict[\"Primary Camera\"] = []\n",
    "flip_dict[\"Secondary Camera\"] = []\n",
    "flip_dict[\"Display Size\"] = []\n",
    "flip_dict[\"Display Resolution\"] = []\n",
    "flip_dict[\"Processor\"] = []\n",
    "flip_dict[\"Processor Cores\"] = []\n",
    "flip_dict[\"Battery Capacity\"] = []\n",
    "flip_dict[\"Battery Type\"] = []\n",
    "flip_dict[\"Price\"] = []\n",
    "flip_dict[\"URL\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data from each url\n",
    "for url in flip_urls:\n",
    "    driver.get(url)    # Saving url                                                     \n",
    "    print(\"Scraping URL = \", url)\n",
    "    flip_dict['URL'].append(url)                                                          # Loading the webpage by url\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        read_more = driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _1FH0tX\"]')     # Button for expanding the specs\n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception Occured. Moving to next page\")\n",
    "    \n",
    "    try:\n",
    "        brand = driver.find_element_by_xpath('//span[@class=\"B_NuCI\"]')      # Extracting Brand from xpath\n",
    "        flip_dict[\"Brand\"].append(brand.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Brand'].append('-')\n",
    "        \n",
    "    try:\n",
    "        price = driver.find_element_by_xpath('//div[@class=\"_30jeq3 _16Jk6d\"]')      # Extracting Price from xpath\n",
    "        flip_dict['Price'].append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Price'].append('-')\n",
    "        \n",
    "    try:\n",
    "        name = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')      # Extracting Name from xpath\n",
    "        flip_dict['Smartphone'].append(name.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Smartphone'].append('-')\n",
    "    \n",
    "    try:\n",
    "        color = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      # Extracting colour from xpath\n",
    "        flip_dict['Colour'].append(color.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Colour'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_size = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[1]/td[2]/ul/li')  # Extracting Display Size from xpath\n",
    "        flip_dict['Display Size'].append(disp_size.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Size'].append('-')\n",
    "    \n",
    "    try:\n",
    "        disp_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/div')\n",
    "        if disp_chk.text != \"Display Features\" : raise NoSuchElementException\n",
    "        disp_res = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][2]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Display Resolution from xpath\n",
    "        flip_dict['Display Resolution'].append(disp_res.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Display Resolution'].append('-')\n",
    "    \n",
    "    try:\n",
    "        pro_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "        if pro_chk.text != \"Processor Type\" : raise NoSuchElementException\n",
    "        processor = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')   # Extracting processor from xpath\n",
    "        Processor.append(processor.text)\n",
    "    except NoSuchElementException:\n",
    "        Processor.append('-')\n",
    "    \n",
    "    try:                                                                                     # Extracting Processor Cores from xpath\n",
    "        core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[1]')\n",
    "        if core_chk.text != \"Processor Core\" :\n",
    "            core_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[1]')\n",
    "            if core_chk.text != \"Processor Core\" : \n",
    "                raise NoSuchElementException\n",
    "            else :\n",
    "                cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[2]/td[2]/ul/li')\n",
    "        else :\n",
    "            cores = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][3]/table[1]/tbody/tr[3]/td[2]/ul/li')\n",
    "        flip_dict['Processor Cores'].append(cores.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Processor Cores'].append('-')\n",
    "    \n",
    "    try:\n",
    "        rom = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[1]/td[2]/ul/li')         # Extracting Storage(ROM) from xpath\n",
    "        flip_dict['Storage(ROM)'].append(rom.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Storage(ROM)'].append('-')\n",
    "    \n",
    "    try:\n",
    "        ram = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                # Extracting RAM from xpath\n",
    "        flip_dict['RAM'].append(ram.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['RAM'].append('-')\n",
    "    \n",
    "    try:                                                                                  \n",
    "        pri_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[2]/td[2]/ul/li')     # Extracting Camera from xpath\n",
    "        flip_dict['Primary Camera'].append(pri_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Primary Camera'].append('-')\n",
    "    \n",
    "    try:                                                                                    # Extracting Secondary Camera from xpath\n",
    "        cam_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[1]')\n",
    "        if cam_chk != \"Secondary Camera\" : \n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[1]').text == \"Secondary Camera\":\n",
    "                sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[5]/td[2]/ul/li')\n",
    "            else :\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            sec_cam = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][5]/table[1]/tbody/tr[6]/td[2]/ul/li')\n",
    "        flip_dict['Secondary Camera'].append(sec_cam.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Secondary Camera'].append('-')\n",
    "        \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :   # Extracting Battery Capacity from xpath\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr/td[2]/ul/li')                \n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :     \n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[1]')\n",
    "                if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "                bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[1]')\n",
    "            if bat_chk.text != \"Battery Capacity\" : raise NoSuchElementException\n",
    "            bat_cap = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr/td[2]/ul/li')                # Extracting Availability from xpath\n",
    "        flip_dict['Battery Capacity'].append(bat_cap.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Capacity'].append('-')\n",
    "    \n",
    "    try:\n",
    "        if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/div').text != \"Battery & Power Features\" :\n",
    "            if driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][9]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            elif driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/div').text == \"Battery & Power Features\" :\n",
    "                bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[1]')\n",
    "                if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "                bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][8]/table/tbody/tr[2]/td[2]/ul/li')\n",
    "            else:\n",
    "                raise NoSuchElementException\n",
    "        else :\n",
    "            bat_chk = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[1]')\n",
    "            if bat_chk.text != \"Battery Type\" : raise NoSuchElementException\n",
    "            bat_typ = driver.find_element_by_xpath('//div[@class=\"_3k-BhJ\"][10]/table/tbody/tr[2]/td[2]/ul/li')             # Extracting Battery Type from xpath\n",
    "        flip_dict['Battery Type'].append(bat_typ.text)\n",
    "    except NoSuchElementException:\n",
    "        flip_dict['Battery Type'].append('-')\n",
    "    \n",
    "                                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "print(len(flip_dict[\"Brand\"]), len(flip_dict[\"Smartphone\"]), len(flip_dict[\"Processor\"]), len(flip_dict[\"Price\"]), len(flip_dict['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Smartphone</th>\n",
       "      <th>Colour</th>\n",
       "      <th>RAM</th>\n",
       "      <th>Storage(ROM)</th>\n",
       "      <th>Primary Camera</th>\n",
       "      <th>Secondary Camera</th>\n",
       "      <th>Display Size</th>\n",
       "      <th>Display Resolution</th>\n",
       "      <th>Processor</th>\n",
       "      <th>Processor Cores</th>\n",
       "      <th>Battery Capacity</th>\n",
       "      <th>Battery Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand, Smartphone, Colour, RAM, Storage(ROM), Primary Camera, Secondary Camera, Display Size, Display Resolution, Processor, Processor Cores, Battery Capacity, Battery Type, Price, URL]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_df = pd.DataFrame.from_dict(flip_dict)\n",
    "flip_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the name of place:new delhi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:443: UserWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  warnings.warn(\"find_element_by_* commands are deprecated. Please use find_element() instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Extracted:  https://www.google.co.in/maps/place/%E0%A4%A8%E0%A4%88+%E0%A4%A6%E0%A4%BF%E0%A4%B2%E0%A5%8D%E0%A4%B2%E0%A5%80,+%E0%A4%A6%E0%A4%BF%E0%A4%B2%E0%A5%8D%E0%A4%B2%E0%A5%80,+%E0%A4%AD%E0%A4%BE%E0%A4%B0%E0%A4%A4/@28.5272803,77.0688983,11z/data=!3m1!4b1!4m5!3m4!1s0x390cfd5b347eb62d:0x52c2b7494e204dce!8m2!3d28.6139391!4d77.2090212\n",
      "Latitude = 28.5272803, Longitude = 77.0688983\n"
     ]
    }
   ],
   "source": [
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening google maps\n",
    "url = \"https://www.google.co.in/maps\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "searchcity=input('enter the name of place:')\n",
    "search = driver.find_element_by_id(\"searchboxinput\")                      \n",
    "search.clear()                                                            \n",
    "time.sleep(2)\n",
    "search.send_keys(searchcity)                                                     \n",
    "button = driver.find_element_by_id(\"searchbox-searchbutton\")               \n",
    "button.click()                                                            \n",
    "time.sleep(3)\n",
    "\n",
    "try:\n",
    "    url_string = driver.current_url\n",
    "    print(\"URL Extracted: \", url_string)\n",
    "    lat_lng = re.findall(r'@(.*)data',url_string)\n",
    "    if len(lat_lng):\n",
    "        lat_lng_list = lat_lng[0].split(\",\")\n",
    "        if len(lat_lng_list)>=2:\n",
    "            lat = lat_lng_list[0]\n",
    "            lng = lat_lng_list[1]\n",
    "        print(\"Latitude = {}, Longitude = {}\".format(lat, lng))\n",
    "\n",
    "except Exception as e:\n",
    "        print(\"Error: \", str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Q6.\tWrite a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:483: UserWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  warnings.warn(\"find_element_by_* commands are deprecated. Please use find_element() instead\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:502: UserWarning: find_elements_by_* commands are deprecated. Please use find_elements() instead\n",
      "  warnings.warn(\"find_elements_by_* commands are deprecated. Please use find_elements() instead\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Startup Name</th>\n",
       "      <th>Industry/Vertical</th>\n",
       "      <th>Sub-Vertical</th>\n",
       "      <th>Location</th>\n",
       "      <th>Investor</th>\n",
       "      <th>Investment Type</th>\n",
       "      <th>Amount(in USD)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Walmart Inc</td>\n",
       "      <td>M&amp;A</td>\n",
       "      <td>1,200,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Vedantu</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Coatue Management</td>\n",
       "      <td>Series D</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/07/2020</td>\n",
       "      <td>Crio</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Learning Platform for Developers</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>021 Capital</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>934,160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>goDutch</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Group Payments</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Matrix India,Y Combinator, Global Founders Cap...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,700,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Mystifly</td>\n",
       "      <td>Airfare Marketplace</td>\n",
       "      <td>Ticketing, Airline Retailing, and Post-Ticketi...</td>\n",
       "      <td>Singapore and Bangalore</td>\n",
       "      <td>Recruit Co. Ltd.</td>\n",
       "      <td>pre-Series B</td>\n",
       "      <td>3,300,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>JetSynthesys</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Gaming and Entertainment</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Adar Poonawalla and Kris Gopalakrishnan.</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10/07/2020</td>\n",
       "      <td>gigIndia</td>\n",
       "      <td>Marketplace</td>\n",
       "      <td>Crowd Sourcing, Freelance</td>\n",
       "      <td>Pune</td>\n",
       "      <td>Incubate Fund India and Beyond Next Ventures</td>\n",
       "      <td>pre-Series A</td>\n",
       "      <td>974,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15/07/2020</td>\n",
       "      <td>PumPumPum</td>\n",
       "      <td>Automotive Rental</td>\n",
       "      <td>Used Car-leasing platform</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Early Adapters Syndicate</td>\n",
       "      <td>Seed</td>\n",
       "      <td>292,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14/07/2020</td>\n",
       "      <td>FLYX</td>\n",
       "      <td>OTT Player</td>\n",
       "      <td>Streaming Social Network</td>\n",
       "      <td>New York and Delhi</td>\n",
       "      <td>Raj Mishra, founder of AIT Global Inc</td>\n",
       "      <td>pre-Seed</td>\n",
       "      <td>200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13/07/2020</td>\n",
       "      <td>Open Appliances Pvt. Ltd.</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Internet-of-Things Security Solutions</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Unicorn India Ventures</td>\n",
       "      <td>Venture-Series Unknown</td>\n",
       "      <td>500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15/08/2020</td>\n",
       "      <td>Practo</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>Health care and Wellness</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>A1A Company</td>\n",
       "      <td>Series F</td>\n",
       "      <td>32,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Medlife</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Prasid Uno Family Trust and SC Credit Fund</td>\n",
       "      <td></td>\n",
       "      <td>23,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>HungerBox</td>\n",
       "      <td>FoodTech</td>\n",
       "      <td>Online Food Delivery Service</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>One97, Sabre Partners Trust, Pratithi Investme...</td>\n",
       "      <td>Series D1</td>\n",
       "      <td>1,560,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04/08/2020</td>\n",
       "      <td>Dunzo</td>\n",
       "      <td>Hyper-local Logistics</td>\n",
       "      <td>Online Delivery Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Existing Backers</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>30,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11/08/2020</td>\n",
       "      <td>Terra.do</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Climate School, E-learning</td>\n",
       "      <td>Stanford, California,</td>\n",
       "      <td>Stanford Angels and Entrepreneurs (India), BEE...</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12/08/2020</td>\n",
       "      <td>Classplus</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>E-learning, Online Tutoring</td>\n",
       "      <td>Noida</td>\n",
       "      <td>Falcon Edge</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>upto 15,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14/08/2020</td>\n",
       "      <td>Niyo</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Niyo Solutions Inc.</td>\n",
       "      <td></td>\n",
       "      <td>6,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10/08/2020</td>\n",
       "      <td>ZestMoney</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Primrose Hills Ventures</td>\n",
       "      <td></td>\n",
       "      <td>10,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>07/08/2020</td>\n",
       "      <td>FreshToHome</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Food Delivery</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Ascent Capital</td>\n",
       "      <td>Venture</td>\n",
       "      <td>16,200,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>13/08/2020</td>\n",
       "      <td>Eduvanz</td>\n",
       "      <td>FinTech</td>\n",
       "      <td>Financial Services</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Sequoia India, Unitus</td>\n",
       "      <td>Series A</td>\n",
       "      <td>5,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>08/09/2020</td>\n",
       "      <td>Byju’s</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Tutoring</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Silver Lake, Tiger Global, General Atlantic an...</td>\n",
       "      <td>Private Equity</td>\n",
       "      <td>500,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>12/09/2020</td>\n",
       "      <td>mCaffeine</td>\n",
       "      <td>Personal Care</td>\n",
       "      <td>Skincare &amp; Haircare</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Amicus Capital Private Equity I LLP, Amicus Ca...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>3,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Qshala</td>\n",
       "      <td>EduTech</td>\n",
       "      <td>Online Curiosity Platform for Kids</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Rainmatter Capital</td>\n",
       "      <td>Angel</td>\n",
       "      <td>370,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>02/09/2020</td>\n",
       "      <td>Winzo</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>Online Gaming</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Kalaari Capital Partners, IndigoEdge Managemen...</td>\n",
       "      <td>Series B</td>\n",
       "      <td>15,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>09/09/2020</td>\n",
       "      <td>Hippo Video</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Video Customer Experience(CX) Platform</td>\n",
       "      <td>Newark, Delaware, United States of Amercia</td>\n",
       "      <td>Alpha Wave Incubation, Exfinity Venture Partne...</td>\n",
       "      <td>Series A</td>\n",
       "      <td>4,500,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>Melorra</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Jewelry Store</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Shadow Holdings, Lightbox.</td>\n",
       "      <td>Debt Financing</td>\n",
       "      <td>upto 8,900,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>07/09/2020</td>\n",
       "      <td>1mg</td>\n",
       "      <td>E-commerce</td>\n",
       "      <td>Online Pharmacy</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Gaja Capital, Tata Capital, Partners Group</td>\n",
       "      <td>In Progress</td>\n",
       "      <td>100,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>mfine</td>\n",
       "      <td>HealthTech</td>\n",
       "      <td>On-Demand Healthcare Services</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Caretech Pte Inc</td>\n",
       "      <td>Series B</td>\n",
       "      <td>5,400,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>31/08/2020</td>\n",
       "      <td>Apna</td>\n",
       "      <td>Human Resources</td>\n",
       "      <td>Recruitment Platform</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>Lightspeed India and Sequoia Capital India</td>\n",
       "      <td>Series A</td>\n",
       "      <td>8,000,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>03/09/2020</td>\n",
       "      <td>Railofy</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>WL &amp; RAC protection platform</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>Chiratae Ventures</td>\n",
       "      <td>Seed</td>\n",
       "      <td>950,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date               Startup Name  \\\n",
       "0   15/07/2020                   Flipkart   \n",
       "1   16/07/2020                    Vedantu   \n",
       "2   16/07/2020                       Crio   \n",
       "3   14/07/2020                    goDutch   \n",
       "4   13/07/2020                   Mystifly   \n",
       "5   09/07/2020               JetSynthesys   \n",
       "6   10/07/2020                   gigIndia   \n",
       "7   15/07/2020                  PumPumPum   \n",
       "8   14/07/2020                       FLYX   \n",
       "9   13/07/2020  Open Appliances Pvt. Ltd.   \n",
       "10  15/08/2020                     Practo   \n",
       "11  13/08/2020                    Medlife   \n",
       "12  13/08/2020                  HungerBox   \n",
       "13  04/08/2020                      Dunzo   \n",
       "14  11/08/2020                   Terra.do   \n",
       "15  12/08/2020                  Classplus   \n",
       "16  14/08/2020                       Niyo   \n",
       "17  10/08/2020                  ZestMoney   \n",
       "18  07/08/2020                FreshToHome   \n",
       "19  13/08/2020                    Eduvanz   \n",
       "20  08/09/2020                     Byju’s   \n",
       "21  12/09/2020                  mCaffeine   \n",
       "22  09/09/2020                     Qshala   \n",
       "23  02/09/2020                      Winzo   \n",
       "24  09/09/2020                Hippo Video   \n",
       "25  07/09/2020                    Melorra   \n",
       "26  07/09/2020                        1mg   \n",
       "27  31/08/2020                      mfine   \n",
       "28  31/08/2020                       Apna   \n",
       "29  03/09/2020                    Railofy   \n",
       "\n",
       "                         Industry/Vertical  \\\n",
       "0                               E-commerce   \n",
       "1                                  EduTech   \n",
       "2                                  EduTech   \n",
       "3                                  FinTech   \n",
       "4                      Airfare Marketplace   \n",
       "5                 Gaming and Entertainment   \n",
       "6                              Marketplace   \n",
       "7                        Automotive Rental   \n",
       "8                               OTT Player   \n",
       "9                   Information Technology   \n",
       "10                              HealthTech   \n",
       "11                              E-commerce   \n",
       "12                                FoodTech   \n",
       "13                   Hyper-local Logistics   \n",
       "14                                 EduTech   \n",
       "15                                 EduTech   \n",
       "16                                 FinTech   \n",
       "17                                 FinTech   \n",
       "18                              E-commerce   \n",
       "19                                 FinTech   \n",
       "20                                 EduTech   \n",
       "21                           Personal Care   \n",
       "22                                 EduTech   \n",
       "23                           Online Gaming   \n",
       "24  Video Customer Experience(CX) Platform   \n",
       "25                              E-commerce   \n",
       "26                              E-commerce   \n",
       "27                              HealthTech   \n",
       "28                         Human Resources   \n",
       "29                          Transportation   \n",
       "\n",
       "                                         Sub-Vertical  \\\n",
       "0                                          E-commerce   \n",
       "1                                     Online Tutoring   \n",
       "2                    Learning Platform for Developers   \n",
       "3                                      Group Payments   \n",
       "4   Ticketing, Airline Retailing, and Post-Ticketi...   \n",
       "5                            Gaming and Entertainment   \n",
       "6                           Crowd Sourcing, Freelance   \n",
       "7                           Used Car-leasing platform   \n",
       "8                            Streaming Social Network   \n",
       "9               Internet-of-Things Security Solutions   \n",
       "10                           Health care and Wellness   \n",
       "11                                    Online Pharmacy   \n",
       "12                       Online Food Delivery Service   \n",
       "13                           Online Delivery Services   \n",
       "14                  Online Climate School, E-learning   \n",
       "15                        E-learning, Online Tutoring   \n",
       "16                                 Financial Services   \n",
       "17                                 Financial Services   \n",
       "18                                      Food Delivery   \n",
       "19                                 Financial Services   \n",
       "20                                    Online Tutoring   \n",
       "21                                Skincare & Haircare   \n",
       "22                 Online Curiosity Platform for Kids   \n",
       "23                                      Online Gaming   \n",
       "24             Video Customer Experience(CX) Platform   \n",
       "25                               Online Jewelry Store   \n",
       "26                                    Online Pharmacy   \n",
       "27                      On-Demand Healthcare Services   \n",
       "28                               Recruitment Platform   \n",
       "29                       WL & RAC protection platform   \n",
       "\n",
       "                                      Location  \\\n",
       "0                                    Bangalore   \n",
       "1                                    Bangalore   \n",
       "2                                    Bangalore   \n",
       "3                                       Mumbai   \n",
       "4                      Singapore and Bangalore   \n",
       "5                                         Pune   \n",
       "6                                         Pune   \n",
       "7                                      Gurgaon   \n",
       "8                           New York and Delhi   \n",
       "9                                    Bangalore   \n",
       "10                                   Bangalore   \n",
       "11                                   Bangalore   \n",
       "12                                   Bangalore   \n",
       "13                                   Bangalore   \n",
       "14                       Stanford, California,   \n",
       "15                                       Noida   \n",
       "16                                   Bangalore   \n",
       "17                                   Bangalore   \n",
       "18                                   Bangalore   \n",
       "19                                      Mumbai   \n",
       "20                                   Bangalore   \n",
       "21                                      Mumbai   \n",
       "22                                   Bangalore   \n",
       "23                                   New Delhi   \n",
       "24  Newark, Delaware, United States of Amercia   \n",
       "25                                   Bangalore   \n",
       "26                                     Gurgaon   \n",
       "27                                   Bangalore   \n",
       "28                                   Bangalore   \n",
       "29                                      Mumbai   \n",
       "\n",
       "                                             Investor         Investment Type  \\\n",
       "0                                         Walmart Inc                     M&A   \n",
       "1                                   Coatue Management                Series D   \n",
       "2                                         021 Capital            pre-Series A   \n",
       "3   Matrix India,Y Combinator, Global Founders Cap...                    Seed   \n",
       "4                                    Recruit Co. Ltd.            pre-Series B   \n",
       "5            Adar Poonawalla and Kris Gopalakrishnan.  Venture-Series Unknown   \n",
       "6        Incubate Fund India and Beyond Next Ventures            pre-Series A   \n",
       "7                            Early Adapters Syndicate                    Seed   \n",
       "8               Raj Mishra, founder of AIT Global Inc                pre-Seed   \n",
       "9                              Unicorn India Ventures  Venture-Series Unknown   \n",
       "10                                        A1A Company                Series F   \n",
       "11         Prasid Uno Family Trust and SC Credit Fund                           \n",
       "12  One97, Sabre Partners Trust, Pratithi Investme...               Series D1   \n",
       "13                                   Existing Backers             In Progress   \n",
       "14  Stanford Angels and Entrepreneurs (India), BEE...                    Seed   \n",
       "15                                        Falcon Edge             In Progress   \n",
       "16                                Niyo Solutions Inc.                           \n",
       "17                            Primrose Hills Ventures                           \n",
       "18                                     Ascent Capital                 Venture   \n",
       "19                              Sequoia India, Unitus                Series A   \n",
       "20  Silver Lake, Tiger Global, General Atlantic an...          Private Equity   \n",
       "21  Amicus Capital Private Equity I LLP, Amicus Ca...                Series B   \n",
       "22                                 Rainmatter Capital                   Angel   \n",
       "23  Kalaari Capital Partners, IndigoEdge Managemen...                Series B   \n",
       "24  Alpha Wave Incubation, Exfinity Venture Partne...                Series A   \n",
       "25                         Shadow Holdings, Lightbox.          Debt Financing   \n",
       "26         Gaja Capital, Tata Capital, Partners Group             In Progress   \n",
       "27                                   Caretech Pte Inc                Series B   \n",
       "28         Lightspeed India and Sequoia Capital India                Series A   \n",
       "29                                  Chiratae Ventures                    Seed   \n",
       "\n",
       "     Amount(in USD)  \n",
       "0     1,200,000,000  \n",
       "1       100,000,000  \n",
       "2           934,160  \n",
       "3         1,700,000  \n",
       "4         3,300,000  \n",
       "5           400,000  \n",
       "6           974,200  \n",
       "7           292,800  \n",
       "8           200,000  \n",
       "9           500,000  \n",
       "10       32,000,000  \n",
       "11       23,000,000  \n",
       "12        1,560,000  \n",
       "13       30,000,000  \n",
       "14        1,400,000  \n",
       "15  upto 15,000,000  \n",
       "16        6,000,000  \n",
       "17       10,670,000  \n",
       "18       16,200,000  \n",
       "19        5,000,000  \n",
       "20      500,000,000  \n",
       "21        3,000,000  \n",
       "22          370,000  \n",
       "23       15,500,000  \n",
       "24        4,500,000  \n",
       "25   upto 8,900,000  \n",
       "26      100,000,000  \n",
       "27        5,400,000  \n",
       "28        8,000,000  \n",
       "29          950,000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening trak.in\n",
    "url = \"https://trak.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "button = driver.find_element_by_xpath('//li[@id=\"menu-item-51510\"]/a').get_attribute('href')\n",
    "driver.get(button)\n",
    "\n",
    "# Empty lists\n",
    "fund_dict = {}\n",
    "fund_dict['Date'] = []\n",
    "fund_dict['Startup Name'] = []\n",
    "fund_dict['Industry/Vertical'] = []\n",
    "fund_dict['Sub-Vertical'] = []\n",
    "fund_dict['Location'] = []\n",
    "fund_dict['Investor'] = []\n",
    "fund_dict['Investment Type'] = []\n",
    "fund_dict['Amount(in USD)'] = []\n",
    "\n",
    "\n",
    "for i in range(48,51):\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        fund_dict['Date'].append(d.text)\n",
    "\n",
    "    # Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        fund_dict['Startup Name'].append(n.text)\n",
    "    \n",
    "    # Industry/Vertical\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        fund_dict['Industry/Vertical'].append(n.text)\n",
    "    \n",
    "    # Sub-Vertical\n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        fund_dict['Sub-Vertical'].append(s.text)\n",
    "\n",
    "    # Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        fund_dict['Location'].append(l.text)\n",
    "    \n",
    "    # Investor\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        fund_dict['Investor'].append(n.text)\n",
    "    \n",
    "    # Investment Type\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        fund_dict['Investment Type'].append(n.text)\n",
    "    \n",
    "    # Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        fund_dict['Amount(in USD)'].append(a.text)\n",
    "    \n",
    "fund_df = pd.DataFrame(fund_dict)\n",
    "fund_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q7. Write a program to scrap all the available details of top 10 gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>price</th>\n",
       "      <th>OS</th>\n",
       "      <th>Display</th>\n",
       "      <th>HDD</th>\n",
       "      <th>RAM</th>\n",
       "      <th>processor</th>\n",
       "      <th>weight</th>\n",
       "      <th>Dimension</th>\n",
       "      <th>Graphical processor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALIENWARE AREA 51M R2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>17.3\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10TH GENERATION INTEL® CORE™ I7-10700 | 2.90 GHZ</td>\n",
       "      <td>4.1</td>\n",
       "      <td>27.65 x 402.6 x 319.14</td>\n",
       "      <td>Intel® UHD Graphics 630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALIENWARE M15 R3</td>\n",
       "      <td>₹341990</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (3840 X 2160)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10TH GENERATION INTEL® CORE™ I9-10980HK | NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS ROG STRIX SCAR 15</td>\n",
       "      <td>N/A</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>AMD RYZEN™ 9 5900HX | 3.3 GHZ</td>\n",
       "      <td>2.30</td>\n",
       "      <td>35.4 x 25.9 x 2.26</td>\n",
       "      <td>NVIDIA® GeForce RTX™ 3070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS ROG ZEPHYRUS G14</td>\n",
       "      <td>₹164990</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>14\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>AMD 3RD GENERATION RYZEN 9 | 3.3 GHZ</td>\n",
       "      <td>1.65</td>\n",
       "      <td>32.5 x 22.1 x 1.8</td>\n",
       "      <td>NVIDIA GeForce RTX 2060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LENOVO LEGION 5I</td>\n",
       "      <td>₹71990</td>\n",
       "      <td>WINDOWS 10 PRO</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>1 TB SSD</td>\n",
       "      <td>16 GBGB DDR4</td>\n",
       "      <td>10TH GENERATION INTEL® CORE™ I5-10300H | 2.50 GHZ</td>\n",
       "      <td>2.3</td>\n",
       "      <td>363.06 x 259.61 x 23.57</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650 4GB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASUS ROG ZEPHYRUS DUO 15</td>\n",
       "      <td>₹199990</td>\n",
       "      <td>WINDOWS 10</td>\n",
       "      <td>15.6\" (3840 X 1100)</td>\n",
       "      <td>512 GB SSD</td>\n",
       "      <td>4 GBGB DDR4</td>\n",
       "      <td>INTEL CORE I7 10TH GEN 10875H | NA</td>\n",
       "      <td>2.4</td>\n",
       "      <td>268.30 x 360.00 x 20.90</td>\n",
       "      <td>NVIDIA GeForce RTX 2070 Max-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ACER ASPIRE 7 GAMING</td>\n",
       "      <td>₹56990</td>\n",
       "      <td>WINDOWS 10 HOME</td>\n",
       "      <td>15.6\" (1920 X 1080)</td>\n",
       "      <td>512 GB SSD</td>\n",
       "      <td>8 GBGB DDR4</td>\n",
       "      <td>AMD RYZEN™ 5-5500U HEXA-CORE | NA</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.29 x 36.3 x 25.4</td>\n",
       "      <td>NVIDIA® GeForce® GTX 1650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Name    price               OS              Display  \\\n",
       "0     ALIENWARE AREA 51M R2      N/A  WINDOWS 10 HOME  17.3\" (1920 X 1080)   \n",
       "1          ALIENWARE M15 R3  ₹341990  WINDOWS 10 HOME  15.6\" (3840 X 2160)   \n",
       "2    ASUS ROG STRIX SCAR 15      N/A  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "3     ASUS ROG ZEPHYRUS G14  ₹164990  WINDOWS 10 HOME    14\" (1920 X 1080)   \n",
       "4          LENOVO LEGION 5I   ₹71990   WINDOWS 10 PRO  15.6\" (1920 X 1080)   \n",
       "5  ASUS ROG ZEPHYRUS DUO 15  ₹199990       WINDOWS 10  15.6\" (3840 X 1100)   \n",
       "6      ACER ASPIRE 7 GAMING   ₹56990  WINDOWS 10 HOME  15.6\" (1920 X 1080)   \n",
       "\n",
       "          HDD           RAM  \\\n",
       "0    1 TB SSD  16 GBGB DDR4   \n",
       "1    1 TB SSD  16 GBGB DDR4   \n",
       "2    1 TB SSD  16 GBGB DDR4   \n",
       "3    1 TB SSD  16 GBGB DDR4   \n",
       "4    1 TB SSD  16 GBGB DDR4   \n",
       "5  512 GB SSD   4 GBGB DDR4   \n",
       "6  512 GB SSD   8 GBGB DDR4   \n",
       "\n",
       "                                           processor weight  \\\n",
       "0   10TH GENERATION INTEL® CORE™ I7-10700 | 2.90 GHZ    4.1   \n",
       "1       10TH GENERATION INTEL® CORE™ I9-10980HK | NA     NA   \n",
       "2                      AMD RYZEN™ 9 5900HX | 3.3 GHZ   2.30   \n",
       "3               AMD 3RD GENERATION RYZEN 9 | 3.3 GHZ   1.65   \n",
       "4  10TH GENERATION INTEL® CORE™ I5-10300H | 2.50 GHZ    2.3   \n",
       "5                 INTEL CORE I7 10TH GEN 10875H | NA    2.4   \n",
       "6                  AMD RYZEN™ 5-5500U HEXA-CORE | NA   2.15   \n",
       "\n",
       "                 Dimension            Graphical processor  \n",
       "0   27.65 x 402.6 x 319.14        Intel® UHD Graphics 630  \n",
       "1                       NA                             NA  \n",
       "2       35.4 x 25.9 x 2.26      NVIDIA® GeForce RTX™ 3070  \n",
       "3        32.5 x 22.1 x 1.8        NVIDIA GeForce RTX 2060  \n",
       "4  363.06 x 259.61 x 23.57  NVIDIA® GeForce® GTX 1650 4GB  \n",
       "5  268.30 x 360.00 x 20.90  NVIDIA GeForce RTX 2070 Max-Q  \n",
       "6       2.29 x 36.3 x 25.4      NVIDIA® GeForce® GTX 1650  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "\n",
    "# opening www.digit.in\n",
    "url = \"https://www.digit.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "top_10=driver.find_element_by_xpath(\"/html/body/div[1]/div[2]/div[4]/ul/li[4]/a\")\n",
    "top_10.click()\n",
    "\n",
    "time.sleep(2)\n",
    "#clicking on laptops option\n",
    "laptops=driver.find_element_by_xpath(\"/html/body/div[3]/div/div/div[2]/div[5]/div[1]/div/button[2]\")\n",
    "laptops.click()\n",
    "\n",
    "time.sleep(2)\n",
    "best_gaming=driver.find_element_by_xpath(\"//div[@id='laptops']//div[3]//a\")\n",
    "driver.get(best_gaming.get_attribute('href'))\n",
    "\n",
    "#intialising lists\n",
    "name = []\n",
    "Price = []\n",
    "OS = []\n",
    "display = []\n",
    "processor = []\n",
    "HDD = []\n",
    "RAM = []\n",
    "weight = []\n",
    "dimension = []\n",
    "GPU = []\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping names\n",
    "names=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in names:\n",
    "    name.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping operating system\n",
    "os=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in os:\n",
    "    OS.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping display\n",
    "displays=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in displays:\n",
    "    display.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "#scraping processor\n",
    "processors=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[3]/div/div\")\n",
    "for i in processors:\n",
    "    processor.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping memory\n",
    "memories=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "for i in memories:\n",
    "    HDD.append(i.text.split(\"/\")[0])\n",
    "    RAM.append(i.text.split(\"/\")[1])\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping weight\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[7]/td[3]\")\n",
    "for i in weights:\n",
    "    weight.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping dimension\n",
    "dimension=[]\n",
    "dimensions=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[8]/td[3]\") \n",
    "for i in dimensions:\n",
    "    dimension.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "# scraping graphical processor\n",
    "GPUs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[9]/td[3]\") \n",
    "for i in GPUs:\n",
    "    GPU.append(i.text)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "#scraping price\n",
    "price=driver.find_elements_by_xpath(\"//table[@id='summtable']//tr//td[3]\")\n",
    "for i in price:\n",
    "    Price.append(i.text)\n",
    "\n",
    "#Make data frame\n",
    "df=pd.DataFrame({\"Name\":name,\n",
    "                \"price\":Price,\n",
    "                \"OS\":OS,\n",
    "                \"Display\":display,\n",
    "                \"HDD\":HDD,\n",
    "                 \"RAM\":RAM,\n",
    "                \"processor\":processor,\n",
    "                \"weight\":weight,\n",
    "                \"Dimension\":dimension,\n",
    "                \"Graphical processor\":GPU})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Gaming laptops_digit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q8- Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Net Worth</th>\n",
       "      <th>Age</th>\n",
       "      <th>Country of Citizenship</th>\n",
       "      <th>Source</th>\n",
       "      <th>Industry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Jeff Bezos</td>\n",
       "      <td>$177 B</td>\n",
       "      <td>57</td>\n",
       "      <td>United States</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>Elon Musk</td>\n",
       "      <td>$151 B</td>\n",
       "      <td>49</td>\n",
       "      <td>United States</td>\n",
       "      <td>Tesla, SpaceX</td>\n",
       "      <td>Automotive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Bernard Arnault &amp; family</td>\n",
       "      <td>$150 B</td>\n",
       "      <td>72</td>\n",
       "      <td>France</td>\n",
       "      <td>LVMH</td>\n",
       "      <td>Fashion &amp; Retail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>Bill Gates</td>\n",
       "      <td>$124 B</td>\n",
       "      <td>65</td>\n",
       "      <td>United States</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>Mark Zuckerberg</td>\n",
       "      <td>$97 B</td>\n",
       "      <td>36</td>\n",
       "      <td>United States</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2750</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Daniel Yong Zhang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>49</td>\n",
       "      <td>China</td>\n",
       "      <td>e-commerce</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2751</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhang Yuqiang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>65</td>\n",
       "      <td>China</td>\n",
       "      <td>Fiberglass</td>\n",
       "      <td>Manufacturing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2752</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhao Meiguang</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>gold mining</td>\n",
       "      <td>Metals &amp; Mining</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2753</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhong Naixiong</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>58</td>\n",
       "      <td>China</td>\n",
       "      <td>conglomerate</td>\n",
       "      <td>Diversified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2754</th>\n",
       "      <td>2674.</td>\n",
       "      <td>Zhou Wei family</td>\n",
       "      <td>$1 B</td>\n",
       "      <td>54</td>\n",
       "      <td>China</td>\n",
       "      <td>Software</td>\n",
       "      <td>Technology</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2755 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Rank                      Name Net Worth Age Country of Citizenship  \\\n",
       "0        1.                Jeff Bezos    $177 B  57          United States   \n",
       "1        2.                 Elon Musk    $151 B  49          United States   \n",
       "2        3.  Bernard Arnault & family    $150 B  72                 France   \n",
       "3        4.                Bill Gates    $124 B  65          United States   \n",
       "4        5.           Mark Zuckerberg     $97 B  36          United States   \n",
       "...     ...                       ...       ...  ..                    ...   \n",
       "2750  2674.         Daniel Yong Zhang      $1 B  49                  China   \n",
       "2751  2674.             Zhang Yuqiang      $1 B  65                  China   \n",
       "2752  2674.             Zhao Meiguang      $1 B  58                  China   \n",
       "2753  2674.            Zhong Naixiong      $1 B  58                  China   \n",
       "2754  2674.           Zhou Wei family      $1 B  54                  China   \n",
       "\n",
       "             Source          Industry  \n",
       "0            Amazon        Technology  \n",
       "1     Tesla, SpaceX        Automotive  \n",
       "2              LVMH  Fashion & Retail  \n",
       "3         Microsoft        Technology  \n",
       "4          Facebook        Technology  \n",
       "...             ...               ...  \n",
       "2750     e-commerce        Technology  \n",
       "2751     Fiberglass     Manufacturing  \n",
       "2752    gold mining   Metals & Mining  \n",
       "2753   conglomerate       Diversified  \n",
       "2754       Software        Technology  \n",
       "\n",
       "[2755 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(2)\n",
    "url = \"https://www.forbes.com/?sh=69e6b8c92254\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "button = driver.find_element_by_xpath(\"//button[@class='icon--hamburger']\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "billioners = driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]\")\n",
    "billioners.click()\n",
    "time.sleep(1)\n",
    "world_billioners= driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/li[1]/div[2]/ul/li[2]/a\")\n",
    "world_billioners.click()\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#make empty listes\n",
    "Rank = [] \n",
    "Person_Name = [] \n",
    "total_net_worth = [] \n",
    "Age = []\n",
    "country_of_citizenship = [] \n",
    "Source = []\n",
    "industry = []\n",
    "\n",
    "\n",
    "while(True):\n",
    "    #scraping rank of billionaire\n",
    "    rank= driver.find_elements_by_xpath(\"//div[@class='rank']\")\n",
    "    for i in rank:\n",
    "        Rank.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping name of billionaire\n",
    "    name= driver.find_elements_by_xpath(\"//div[@class='personName']//div\")\n",
    "    for i in name:\n",
    "        Person_Name.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "     #scraping Age of billionaire\n",
    "    age= driver.find_elements_by_xpath(\"//div[@class='age']//div\")\n",
    "    for i in age:\n",
    "        Age.append(i.text)   \n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping citizenship of billionaire    \n",
    "    citizenship= driver.find_elements_by_xpath(\"//div[@class='countryOfCitizenship']\")\n",
    "    for i in citizenship:\n",
    "        country_of_citizenship.append(i.text)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #scraping source of income\n",
    "    source= driver.find_elements_by_xpath(\"//div[@class='source']\")\n",
    "    for i in source:\n",
    "        Source.append(i.text)    \n",
    "    time.sleep(1)\n",
    "        \n",
    "    #scraping Age of billionaire\n",
    "    industries= driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "    for i in industries:\n",
    "        industry.append(i.text)\n",
    "        \n",
    "    #scraping net_worth of billionaire\n",
    "    net_worth= driver.find_elements_by_xpath(\"//div[@class='netWorth']//div[1]\")\n",
    "    for i in net_worth:\n",
    "        total_net_worth.append(i.text)\n",
    "    time.sleep(1)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath(\"//button[@class='pagination-btn pagination-btn--next ']\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break  \n",
    "\n",
    "        \n",
    "Net_Worth = []        \n",
    "for i in range(0,len(total_net_worth),2):\n",
    "        Net_Worth.append(total_net_worth[i])\n",
    "    \n",
    "\n",
    "time.sleep(2)        \n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Rank':Rank,\n",
    "                'Name':Person_Name,\n",
    "                'Net Worth':Net_Worth,\n",
    "                'Age':Age,\n",
    "                'Country of Citizenship':country_of_citizenship,\n",
    "                'Source':Source,\n",
    "                'Industry':industry})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "url = \"https://www.youtube.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "search_bar = driver.find_element_by_id('search')\n",
    "search_bar.send_keys(\"GOT\")  \n",
    "time.sleep(1)\n",
    "search_btn = driver.find_element_by_id(\"search-icon-legacy\")  \n",
    "search_btn.click()\n",
    "time.sleep(1)\n",
    "link_click = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link_click.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    driver.execute_script(\"window.scrollBy(0,10000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comments</th>\n",
       "      <th>Time</th>\n",
       "      <th>Likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Its actually so crazy to me that that 5 second...</td>\n",
       "      <td>10 months ago</td>\n",
       "      <td>3.8K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After watching S8 i just wish that hodor shoul...</td>\n",
       "      <td>5 months ago</td>\n",
       "      <td>567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It’s been a year and a half, and am still wait...</td>\n",
       "      <td>9 months ago</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After 10,000 years the Night King made it past...</td>\n",
       "      <td>9 months ago</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Who else just finished GOT's every seasons! (d...</td>\n",
       "      <td>10 months ago (edited)</td>\n",
       "      <td>1.8K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>\"If u still think this has a happy ending, u h...</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Who all are disappointed when daenarys 🔥Targar...</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>And the noobest writer award goes to GOT season 8</td>\n",
       "      <td>9 months ago</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>DC fans got their remake. What about us?</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>➡️      hotslut.live/1330nlilovedollxxx       ...</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>540 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Comments  \\\n",
       "0    Its actually so crazy to me that that 5 second...   \n",
       "1    After watching S8 i just wish that hodor shoul...   \n",
       "2    It’s been a year and a half, and am still wait...   \n",
       "3    After 10,000 years the Night King made it past...   \n",
       "4    Who else just finished GOT's every seasons! (d...   \n",
       "..                                                 ...   \n",
       "535  \"If u still think this has a happy ending, u h...   \n",
       "536  Who all are disappointed when daenarys 🔥Targar...   \n",
       "537  And the noobest writer award goes to GOT season 8   \n",
       "538           DC fans got their remake. What about us?   \n",
       "539  ➡️      hotslut.live/1330nlilovedollxxx       ...   \n",
       "\n",
       "                       Time Likes  \n",
       "0             10 months ago  3.8K  \n",
       "1              5 months ago   567  \n",
       "2              9 months ago   606  \n",
       "3              9 months ago   314  \n",
       "4    10 months ago (edited)  1.8K  \n",
       "..                      ...   ...  \n",
       "535             2 years ago   179  \n",
       "536              1 week ago     1  \n",
       "537            9 months ago        \n",
       "538            2 months ago     1  \n",
       "539            3 months ago     6  \n",
       "\n",
       "[540 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make empty lists\n",
    "comments = []\n",
    "comment_time = []\n",
    "Time = []\n",
    "Likes = []\n",
    "No_of_Likes = []\n",
    "\n",
    "#scrape comments\n",
    "cm = driver.find_elements_by_id(\"content-text\")\n",
    "for i in cm:\n",
    "    if i.text is None:\n",
    "        comments.append(\"--\")\n",
    "    else:\n",
    "        comments.append(i.text)\n",
    "time.sleep(5)\n",
    "tm = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "for i in tm:\n",
    "    Time.append(i.text)\n",
    "\n",
    "for i in range(0,len(Time),2):\n",
    "    comment_time.append(Time[i])\n",
    "time.sleep(5)\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "    \n",
    "    \n",
    "time.sleep(2)\n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Comments':comments,\n",
    "                'Time':comment_time,\n",
    "                'Likes':No_of_Likes})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\") \n",
    "time.sleep(3)\n",
    "url = \"https://www.hostelworld.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_loc = driver.find_element_by_id('search-input-field')\n",
    "search_loc.send_keys(\"London\")\n",
    "time.sleep(2)\n",
    "london = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/div[1]/div/div[2]/div[4]/div/div[2]/div/div[1]/div/div/ul/li[2]/div')\n",
    "london.click()\n",
    "time.sleep(2)\n",
    "search_btn = driver.find_element_by_id('search-button')\n",
    "search_btn.click()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make empty lists\n",
    "Hostel_Name = []\n",
    "Distance = []\n",
    "overall_review = []\n",
    "total_reviews = []\n",
    "facilities = []\n",
    "price = []\n",
    "Rating = []\n",
    "property_description = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    \n",
    "    # Hostel name\n",
    "    names = driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]')\n",
    "    for name in names:\n",
    "        Hostel_Name.append(name.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Distance from city centre\n",
    "    dis = driver.find_elements_by_xpath('//span[@class=\"description\"]')\n",
    "    for d in dis:\n",
    "        Distance.append(d.text)\n",
    "    time.sleep(2)\n",
    "        \n",
    "    # Overall Review    \n",
    "    review = driver.find_elements_by_xpath('//div[@class=\"keyword\"]//span')\n",
    "    for r in review:\n",
    "        overall_review.append(r.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Total No of reviews     \n",
    "    t_review = driver.find_elements_by_xpath('//div[@class=\"reviews\"]')\n",
    "    for tr in t_review:\n",
    "        total_reviews.append(tr.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # facilities\n",
    "    service = driver.find_elements_by_xpath('//div[@class=\"facilities-label facilities\"]')\n",
    "    for s in service:\n",
    "        facilities.append(s.text)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Prices    \n",
    "    prices = driver.find_elements_by_xpath('//div[@class=\"price-col\"]')\n",
    "    for p in prices:\n",
    "        price.append(p.text)\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "        \n",
    "time.sleep(4)        \n",
    "        \n",
    "# Separate  Privates_From price  and  Dorms_From price\n",
    "Privates_From = []\n",
    "for i in range(0,len(price),2):\n",
    "    Privates_From.append(price[i])\n",
    "time.sleep(2)\n",
    "\n",
    "Dorms_From = []\n",
    "for i in range(1,len(price),2):\n",
    "    Dorms_From.append(price[i])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape Hostels URL\n",
    "hostel_url = []\n",
    "\n",
    "while(True):\n",
    "    urls = driver.find_elements_by_xpath('//h2[@class=\"title title-6\"]//a')\n",
    "    for url in urls:\n",
    "        hostel_url.append(url.get_attribute(\"href\"))\n",
    "    time.sleep(2)    \n",
    "        \n",
    "    try:\n",
    "        next_button = driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]')\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break\n",
    "\n",
    "\n",
    "        \n",
    "Rate = []\n",
    "for page in hostel_url:\n",
    "    driver.get(page)\n",
    "    \n",
    "    # Rating\n",
    "    try:\n",
    "        ratings = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/section/div[6]/div/div[1]/div[1]/div[1]')\n",
    "        Rate.append(ratings.text)\n",
    "    except NoSuchElementException:\n",
    "        Rate.append(\"No Rating\")  \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    # Property Description\n",
    "    try:\n",
    "        pd = driver.find_element_by_xpath('/html/body/div[1]/div/div/div[1]/section/div[6]/div/div[2]/div[2]/div/div[2]')\n",
    "        property_description.append(pd.text)\n",
    "    except NoSuchElementException:\n",
    "        property_description.append(\"No Description\")  \n",
    "\n",
    "    \n",
    "time.sleep(2)        \n",
    "# remove extra data from Rating     \n",
    "all_text = []\n",
    "for i in Rate:\n",
    "    all_text.append(i.split())\n",
    "time.sleep(2)\n",
    "\n",
    "for i in all_text:\n",
    "    Rating.append(i[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hostel Name</th>\n",
       "      <th>Distance from city centre</th>\n",
       "      <th>Overall Review</th>\n",
       "      <th>Total Reviews</th>\n",
       "      <th>Facilities</th>\n",
       "      <th>Privates From Price</th>\n",
       "      <th>Dorms From Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Property Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>St Christopher's Village</td>\n",
       "      <td>Hostel - 1.8km from city centre</td>\n",
       "      <td>Fabulous</td>\n",
       "      <td>10814 Total Reviews</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>Privates From\\nRs3391</td>\n",
       "      <td>Dorms From\\nRs1360</td>\n",
       "      <td>8.9</td>\n",
       "      <td>COVID 19 Policy Update.\\nIn response to Corona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Generator London</td>\n",
       "      <td>Hostel - 3km from city centre</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>6729 Total Reviews</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>Privates From\\nRs8323</td>\n",
       "      <td>Dorms From\\nRs1987</td>\n",
       "      <td>7.5</td>\n",
       "      <td>Generator London is a design hotel-hostel loca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Safestay London Kensington Holland Park</td>\n",
       "      <td>Hostel - 5.9km from city centre</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>1070 Total Reviews</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Dorms From\\nRs1017</td>\n",
       "      <td>7.9</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PubLove @ The Crown, Battersea</td>\n",
       "      <td>Hostel - 4.7km from city centre</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>204 Total Reviews</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>No Privates Available</td>\n",
       "      <td>Dorms From\\nRs1315</td>\n",
       "      <td>7.9</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Leman House</td>\n",
       "      <td>Hostel - 3.6km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>13 Total Reviews</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>Privates From\\nRs5857</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>The Dover</td>\n",
       "      <td>Hotel - 1.9km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>4 Total Reviews</td>\n",
       "      <td>Free WiFi\\nFree Breakfast</td>\n",
       "      <td>Privates From\\nRs7193</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Park Hotel Essex</td>\n",
       "      <td>Hotel - 24.1km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>108 Total Reviews</td>\n",
       "      <td>Free Breakfast\\nFollows Covid-19 sanitation gu...</td>\n",
       "      <td>Privates From\\nRs3597</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Cranbrook Hotel</td>\n",
       "      <td>Hotel - 14.8km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>58 Total Reviews</td>\n",
       "      <td>Free Breakfast\\nFollows Covid-19 sanitation gu...</td>\n",
       "      <td>Privates From\\nRs3597</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>St. Athans</td>\n",
       "      <td>Bed and Breakfast - 2.9km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>234 Total Reviews</td>\n",
       "      <td>Free WiFi\\nFollows Covid-19 sanitation guidance</td>\n",
       "      <td>Privates From\\nRs3877</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Aron Guest House</td>\n",
       "      <td>Bed and Breakfast - 13.1km from city centre</td>\n",
       "      <td>No Rating</td>\n",
       "      <td>26 Total Reviews</td>\n",
       "      <td>Free WiFi</td>\n",
       "      <td>Privates From\\nRs6679</td>\n",
       "      <td>No Dorms Available</td>\n",
       "      <td>No</td>\n",
       "      <td>No Description</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>88 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Hostel Name  \\\n",
       "0                  St Christopher's Village   \n",
       "1                          Generator London   \n",
       "2   Safestay London Kensington Holland Park   \n",
       "3            PubLove @ The Crown, Battersea   \n",
       "4                               Leman House   \n",
       "..                                      ...   \n",
       "83                                The Dover   \n",
       "84                         Park Hotel Essex   \n",
       "85                          Cranbrook Hotel   \n",
       "86                               St. Athans   \n",
       "87                         Aron Guest House   \n",
       "\n",
       "                      Distance from city centre Overall Review  \\\n",
       "0               Hostel - 1.8km from city centre       Fabulous   \n",
       "1                 Hostel - 3km from city centre      Very Good   \n",
       "2               Hostel - 5.9km from city centre      Very Good   \n",
       "3               Hostel - 4.7km from city centre      Very Good   \n",
       "4               Hostel - 3.6km from city centre      No Rating   \n",
       "..                                          ...            ...   \n",
       "83               Hotel - 1.9km from city centre      No Rating   \n",
       "84              Hotel - 24.1km from city centre      No Rating   \n",
       "85              Hotel - 14.8km from city centre      No Rating   \n",
       "86   Bed and Breakfast - 2.9km from city centre      No Rating   \n",
       "87  Bed and Breakfast - 13.1km from city centre      No Rating   \n",
       "\n",
       "          Total Reviews                                         Facilities  \\\n",
       "0   10814 Total Reviews    Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "1    6729 Total Reviews    Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "2    1070 Total Reviews                                          Free WiFi   \n",
       "3     204 Total Reviews    Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "4      13 Total Reviews                                          Free WiFi   \n",
       "..                  ...                                                ...   \n",
       "83      4 Total Reviews                          Free WiFi\\nFree Breakfast   \n",
       "84    108 Total Reviews  Free Breakfast\\nFollows Covid-19 sanitation gu...   \n",
       "85     58 Total Reviews  Free Breakfast\\nFollows Covid-19 sanitation gu...   \n",
       "86    234 Total Reviews    Free WiFi\\nFollows Covid-19 sanitation guidance   \n",
       "87     26 Total Reviews                                          Free WiFi   \n",
       "\n",
       "      Privates From Price    Dorms From Price Rating  \\\n",
       "0   Privates From\\nRs3391  Dorms From\\nRs1360    8.9   \n",
       "1   Privates From\\nRs8323  Dorms From\\nRs1987    7.5   \n",
       "2   No Privates Available  Dorms From\\nRs1017    7.9   \n",
       "3   No Privates Available  Dorms From\\nRs1315    7.9   \n",
       "4   Privates From\\nRs5857  No Dorms Available     No   \n",
       "..                    ...                 ...    ...   \n",
       "83  Privates From\\nRs7193  No Dorms Available     No   \n",
       "84  Privates From\\nRs3597  No Dorms Available     No   \n",
       "85  Privates From\\nRs3597  No Dorms Available     No   \n",
       "86  Privates From\\nRs3877  No Dorms Available     No   \n",
       "87  Privates From\\nRs6679  No Dorms Available     No   \n",
       "\n",
       "                                 Property Description  \n",
       "0   COVID 19 Policy Update.\\nIn response to Corona...  \n",
       "1   Generator London is a design hotel-hostel loca...  \n",
       "2                                      No Description  \n",
       "3                                      No Description  \n",
       "4                                      No Description  \n",
       "..                                                ...  \n",
       "83                                     No Description  \n",
       "84                                     No Description  \n",
       "85                                     No Description  \n",
       "86                                     No Description  \n",
       "87                                     No Description  \n",
       "\n",
       "[88 rows x 9 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe\n",
    "df = pd.DataFrame({'Hostel Name':Hostel_Name,\n",
    "                'Distance from city centre':Distance,\n",
    "                'Overall Review':overall_review[3:],\n",
    "                'Total Reviews':total_reviews,\n",
    "                'Facilities':facilities,\n",
    "                'Privates From Price':Privates_From,\n",
    "                'Dorms From Price':Dorms_From,\n",
    "                'Rating':Rating,\n",
    "                'Property Description':property_description})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
